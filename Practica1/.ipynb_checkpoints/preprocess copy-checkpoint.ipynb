{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Words\n",
    "\n",
    "We have created a .csv file contaning the top 1000 most frequent english word translated into 10 different languages:\n",
    " - English\n",
    " - Catalan\n",
    " - Spanish\n",
    " - German\n",
    " - French\n",
    " - Polish\n",
    " - Portuguese\n",
    " - Russian\n",
    " - Italian\n",
    " - Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Read Data and convert to lowercase strings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower())\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\pyAA\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/data.csv'"
     ]
    }
   ],
   "source": [
    "# Read Data and convert to lowercase strings\n",
    "df = pd.read_csv('data/data.csv', sep=',')\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "We create a new dataset concatenating all words and create a new column containing the language of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as</td>\n",
       "      <td>ang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his</td>\n",
       "      <td>ang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>ang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he</td>\n",
       "      <td>ang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>ang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word lang\n",
       "0    as  ang\n",
       "1   his  ang\n",
       "2  that  ang\n",
       "3    he  ang\n",
       "4   was  ang"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all languages into one Dataframe\n",
    "dfs = list()\n",
    "for lang in df.columns:\n",
    "    df_lang = pd.DataFrame(df[lang])\n",
    "    df_lang['lang'] = lang[:3]\n",
    "    df_lang = df_lang.rename(columns={lang: 'word'})\n",
    "    dfs.append(df_lang)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In this section we create new features to try to have more information for better predictions.\n",
    "\n",
    "We've created different \"types\" of features:\n",
    " - Character counting\n",
    " - Groups of characters\n",
    " - Prefixes and Suffixes\n",
    " - Zipf's Law \n",
    "\n",
    "Having more information about the words will be useful in order to predict the language of the word. \n",
    "\n",
    "We must have in mind that all of our features are subsets of the languages characteristics. Per example, almost every word that contains the letter \"ñ\" is a spanish word, but not all spanish words contain the letter \"ñ\". \n",
    "So this feature will help us to predict the words that *do* contain an \"ñ\" but not the ones that don't. Our objective is to create enough features so they can cover as much of the language's characteristics as possible. This will increase the chances of correctly predicting the language of a word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Word Length Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "df['len'] = df['word'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Special Characters in word\n",
    "\n",
    "We've created different new features based on the characters in the words. \n",
    "This characteristcs are devided into two groups: \n",
    " - *General features:* features like the number of vowels, accents, number of spaces, etc\n",
    " - *Language specific features:* the number of special characters specific to the language, like the number of \"ñ\", \"ç\", \"é\", etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dictionary containing the feature name and the special characters we want to check.\n",
    "dict = {\n",
    "    'vow':'aeiou', \n",
    "    'acc':'àèìòùáéíóú', \n",
    "    'accl':'àèìòù', \n",
    "    'accr':'áéíóú', \n",
    "    'die':'äëïöü',\n",
    "    'cir':'âêîôû', \n",
    "    'ñ':'ñ', \n",
    "    'ç':'ç', \n",
    "    'ale': 'ß', \n",
    "    'rus': 'бвгджзийклмнпрстфцчшщъыьэюя',\n",
    "    'pol': 'ąćęłńóśźż', \n",
    "    'por': 'ãõ', \n",
    "    'sue': 'åäö',\n",
    "    'esp': 'áéíóúü',\n",
    "    'ita': 'àèéìíîòóùú',\n",
    "    'fra': 'àâæçéèêëîïôœùûüÿ',\n",
    "    'ger': 'äöüß',\n",
    "    'cat': 'àèéíïòóúüç',\n",
    "    'num_words': ' ',\n",
    "    \"apos\": \"'\",\n",
    "    \"hyph\": \"-\",\n",
    "    \"rares\": \"kqwxyz\"\n",
    "}\n",
    "\n",
    "# Returns the number of appearances of any char from string in word.\n",
    "def count_special_characters(word: str, string: str) -> int:\n",
    "    num_special_characters=0\n",
    "    for char in word:\n",
    "        if char in string:\n",
    "            num_special_characters+=1\n",
    "    return num_special_characters\n",
    "\n",
    "\n",
    "# For every entry in the dictionary we create that column using the function above.\n",
    "for column in dict:\n",
    "        df[column] = df['word'].apply(lambda row: count_special_characters(row, dict[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for common prefixes in different languages, to try to obtain more information about the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing feature name, and list of the common prefixes in that language.\n",
    "common_prefixes = {\n",
    "    'pre_eng': [\"anti\", \"be\", \"de\", \"dis\", \"en\", \"ex\", \"im\", \"in\", \"non\", \"pre\", \"re\", \"un\"],\n",
    "    'pre_esp': [\"anti\", \"auto\", \"contra\", \"des\", \"en\", \"ex\", \"in\", \"inter\", \"pre\", \"re\", \"sub\", \"trans\"],\n",
    "    'pre_cat': [\"anti\", \"ab\", \"avant\", \"arxi\", \"dia\", \"hemi\", \"auto\", \"contra\", \"des\", \"en\", \"ex\", \"in\", \"inter\", \"pre\", \"re\", \"sub\", \"trans\"],\n",
    "    'pre_ita': [\"auto\", \"dis\", \"en\", \"ex\", \"im\", \"in\", \"ir\", \"mal\", \"per\", \"pre\", \"pro\", \"re\", \"sott\", \"sotto\", \"tran\", \"ab\"],\n",
    "    'pre_fra': [\"anti\", \"auto\", \"co\", \"con\", \"contre\", \"de\", \"des\", \"en\", \"ex\", \"in\", \"inter\", \"mal\", \"pre\", \"pro\", \"re\", \"sub\", \"sur\"],\n",
    "    'pre_por': [\"auto\", \"co\", \"contra\", \"des\", \"em\", \"en\", \"ex\", \"in\", \"inter\", \"pre\", \"pro\", \"re\", \"sub\"],\n",
    "    'pre_ale': [\"be\", \"ein\", \"ent\", \"er\", \"ge\", \"hin\", \"ver\", \"zer\"],\n",
    "    'pre_sue': [\"be\", \"för\", \"in\", \"om\", \"över\", \"under\"],\n",
    "    'pre_pol': [\"przed\", \"nad\", \"na\", \"pod\", \"z\", \"w\"],\n",
    "    'pre_rus': [\"анти\", \"без\", \"в\", \"во\", \"до\", \"за\", \"из\", \"над\", \"пере\", \"под\", \"по\", \"пре\", \"раз\"]\n",
    "}\n",
    "\n",
    "# Returns if word starts with a prefix from prefixes list.\n",
    "def has_prefix(word: str, prefixes: list) -> int:\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# For every entry in the dictionary we create that column using the function above.\n",
    "for column in common_prefixes:\n",
    "        df[column] = df['word'].apply(lambda row: has_prefix(row, common_prefixes[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary to the case above, we check for common suffixes in the different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing feature name, and list of the common suffixes in that language.\n",
    "common_suffixes = {\n",
    "    'suf_eng': [\"able\", \"al\", \"ation\", \"er\", \"est\", \"ful\", \"ing\", \"ion\", \"ive\", \"less\", \"ly\", \"ness\", \"ous\", \"s\", \"y\"],\n",
    "    'suf_esp': [\"ado\", \"ador\", \"aje\", \"anza\", \"ar\", \"ario\", \"ero\", \"iente\", \"illa\", \"ina\", \"izar\", \"oso\", \"ón\", \"udo\", \"er\", \"ir\"],\n",
    "    'suf_cat': [\"ana\", \"aca\", \"ada\", \"al\", \"am\", \"ador\", \"tge\", \"isme\", \"nça\", \"ar\", \"ista\", \"istic\", \"mente\", \"ment\", \"ina\", \"tzar\", \"nça\" \"on\", \"um\", \"ut\", \"uda\", \"er\", \"ir\", \"re\"],\n",
    "    'suf_ita': [\"abile\", \"are\", \"ario\", \"atore\", \"azione\", \"ente\", \"evole\", \"ificare\", \"ivo\", \"izzare\", \"ore\", \"orente\", \"orevole\", \"oso\", \"ura\"],\n",
    "    'suf_fra': [\"age\", \"aille\", \"ance\", \"eau\", \"eux\", \"eur\", \"eurse\", \"ie\", \"iment\", \"ion\", \"ique\", \"isme\", \"iste\", \"ition\", \"ive\", \"oire\", \"ure\", \"y\"],\n",
    "    'suf_por': [\"al\", \"ão\", \"ar\", \"ês\", \"ência\", \"eza\", \"ia\", \"ício\", \"imento\", \"ir\", \"or\", \"oso\", \"ura\"],\n",
    "    'suf_ale': [\"bar\", \"e\", \"ei\", \"er\", \"heit\", \"ich\", \"ig\", \"in\", \"keit\", \"lich\", \"ling\", \"sam\", \"schaft\", \"ung\"],\n",
    "    'suf_sue': [\"ande\", \"are\", \"bar\", \"dom\", \"else\", \"en\", \"eri\", \"het\", \"ing\", \"isk\", \"itet\", \"lig\", \"lighet\", \"ning\", \"ningen\", \"ningar\", \"ningen\"],\n",
    "    'suf_pol': [\"acja\", \"ać\", \"anie\", \"eć\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\", \"enie\"],\n",
    "    'suf_rus': [\"больше\", \"енький\", \"ик\", \"ичка\", \"ок\", \"онок\", \"ушко\", \"ца\", \"чек\", \"шка\", \"шко\", \"ящик\", \"ец\", \"ин\", \"ист\", \"ник\", \"овец\", \"щик\", \"ёнок\", \"ь\"]\n",
    "}\n",
    "\n",
    "# Returns if word ends with a prefix from suffixes list.\n",
    "def has_suffix(word: str, suffixes: list) -> int:\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# For every entry in the dictionary we create that column using the function above.\n",
    "for column in common_suffixes:\n",
    "    df[column] = df['word'].apply(lambda row: has_suffix(row, common_suffixes[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check letter groups in a word based on groups commonly used by each language, we also check for contiguous vowel pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates all the pairs of 2 vowels.\n",
    "def diptongos() -> list:\n",
    "    list=[]\n",
    "    vowels = \"aeiouàèìòùáéíóúäëïöüâêîôû\"\n",
    "    for i in vowels:\n",
    "        for j in vowels:\n",
    "            list.append(i+j)\n",
    "    return list\n",
    "\n",
    "# Dictionary containing feature name, and list of the leetter groups in that language.\n",
    "groups = {\n",
    "        \"pairs_eng\": [\"sh\", \"th\", \"ch\", \"ck\", \"ph\", \"ng\", \"qu\", \"tr\", \"st\", \"wh\", \"tr\"],\n",
    "        \"pairs_cat\": [\"ny\", \"tx\", \"sc\", \"nc\", \"rc\", \"ll\", \"nc\", \"pc\", \"pr\", \"br\", \"fr\", \"ts\", \"ix\",  \"nd\", \"pr\", \"bl\"],\n",
    "        \"pairs_esp\": [\"nd\", \"nt\", \"ch\", \"rr\", \"ll\", \"qu\", \"gu\", \"nc\", \"mb\", \"pr\"],\n",
    "        \"pairs_ger\": [\"tch\", \"ck\", \"ng\", \"qu\", \"tz\", \"ss\", \"st\", \"sp\", \"str\", \"sch\"],\n",
    "        \"pairs_por\": [\"tch\", \"lh\", \"nh\", \"qu\", \"sc\", \"rr\", \"nc\", \"gu\", \"lm\", \"rm\"],\n",
    "        \"pairs_pol\": [\"ch\", \"dz\", \"dł\", \"di\", \"rz\", \"sz\", \" sc\", \"ed\", \"id\"],\n",
    "        \"pairs_ita\": [\"ch\", \"gl\", \"gn\", \"sc\", \"qu\", \"scl\", \"ch\", \"ci\", \"gli\", \"gn\", \"io\", \"la\", \"leu\", \"ii\", \"io\", \"ne\"],\n",
    "        \"pairs_swe\": [\"ch\", \"ck\", \"cid\", \"dt\", \"gg\", \"ll\", \"ng\", \"sk\", \"st\", \"tt\"],\n",
    "        \"pairs_fre\": [\"ch\", \"che\", \"eau\", \"ent\", \"es\", \"ette\", \"eur\", \"iau\", \"ie\", \"in\"],\n",
    "        \"pairs_rus\": [\"бл\", \"вл\", \"гл\", \"дл\", \"жл\", \"зл\", \"кл\", \"лл\", \"мл\", \"нл\", \"пл\", \"рл\", \"сл\", \"тл\", \"фл\", \"хл\", \"цл\", \"чл\", \"шл\", \"щл\"],\n",
    "        \"diptongos\": diptongos()\n",
    "        }\n",
    "\n",
    "def count_group(word: str, groups: list) -> int:\n",
    "    num_groups=0\n",
    "    for group in groups:\n",
    "        if group in word:\n",
    "            num_groups+=1\n",
    "    return num_groups\n",
    "\n",
    "# For every entry in the dictionary we create that column using the function above.\n",
    "for column in groups:\n",
    "        df[column] = df['word'].apply(lambda row: count_group(row, groups[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Word Frequency in each language\n",
    "\n",
    "To do this we use the Zipf's Law. This law states that the frequency of a word is inversely proportional to its rank in the frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = ['en', 'es', 'ca', 'it', 'fr', 'pt', 'de', 'sv', 'pl', 'ru']\n",
    "\n",
    "for code in language_codes:\n",
    "        df[code+'_zipf'] = df['word'].apply(lambda row: wordfreq.zipf_frequency(row, code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizar los idiomas de 0 a 9\n",
    "df['lang'] = df['lang'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lang</th>\n",
       "      <th>len</th>\n",
       "      <th>vow</th>\n",
       "      <th>acc</th>\n",
       "      <th>accl</th>\n",
       "      <th>accr</th>\n",
       "      <th>die</th>\n",
       "      <th>cir</th>\n",
       "      <th>ñ</th>\n",
       "      <th>...</th>\n",
       "      <th>en_zipf</th>\n",
       "      <th>es_zipf</th>\n",
       "      <th>ca_zipf</th>\n",
       "      <th>it_zipf</th>\n",
       "      <th>fr_zipf</th>\n",
       "      <th>pt_zipf</th>\n",
       "      <th>de_zipf</th>\n",
       "      <th>sv_zipf</th>\n",
       "      <th>pl_zipf</th>\n",
       "      <th>ru_zipf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.77</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.26</td>\n",
       "      <td>5.79</td>\n",
       "      <td>6.73</td>\n",
       "      <td>4.57</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.51</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.01</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.56</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.69</td>\n",
       "      <td>5.77</td>\n",
       "      <td>5.87</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.82</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.99</td>\n",
       "      <td>6.49</td>\n",
       "      <td>4.01</td>\n",
       "      <td>5.57</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.01</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.73</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5.62</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.98</td>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.91</td>\n",
       "      <td>4.66</td>\n",
       "      <td>6.18</td>\n",
       "      <td>4.98</td>\n",
       "      <td>6.71</td>\n",
       "      <td>5.17</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.93</td>\n",
       "      <td>5.98</td>\n",
       "      <td>4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>are</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.74</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.42</td>\n",
       "      <td>4.42</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.85</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.52</td>\n",
       "      <td>4.52</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>they</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>be</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.21</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.27</td>\n",
       "      <td>4.47</td>\n",
       "      <td>4.72</td>\n",
       "      <td>5.04</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>at</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.70</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.82</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>one</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.47</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.63</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.62</td>\n",
       "      <td>5.49</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.71</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.83</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.23</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.82</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.42</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.48</td>\n",
       "      <td>4.49</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>from</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.63</td>\n",
       "      <td>4.18</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.47</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>by</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.66</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.52</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.94</td>\n",
       "      <td>6.26</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hot</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.16</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.09</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.32</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.26</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>but</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.63</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.90</td>\n",
       "      <td>5.28</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.28</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  lang  len  vow  acc  accl  accr  die  cir  ñ  ...  en_zipf  es_zipf  \\\n",
       "0     as     1    2    1    0     0     0    0    0  0  ...     6.77     4.66   \n",
       "1    his     1    3    1    0     0     0    0    0  0  ...     6.51     3.63   \n",
       "2   that     1    4    1    0     0     0    0    0  0  ...     7.01     4.12   \n",
       "3     he     1    2    1    0     0     0    0    0  0  ...     6.69     5.77   \n",
       "4    was     1    3    1    0     0     0    0    0  0  ...     6.82     3.84   \n",
       "5    for     1    3    1    0     0     0    0    0  0  ...     7.01     4.66   \n",
       "6     on     1    2    1    0     0     0    0    0  0  ...     6.91     4.66   \n",
       "7    are     1    3    2    0     0     0    0    0  0  ...     6.74     4.07   \n",
       "8   with     1    4    1    0     0     0    0    0  0  ...     6.85     4.20   \n",
       "9   they     1    4    1    0     0     0    0    0  0  ...     6.50     3.61   \n",
       "10    be     1    2    1    0     0     0    0    0  0  ...     6.79     4.21   \n",
       "11    at     1    2    1    0     0     0    0    0  0  ...     6.70     4.20   \n",
       "12   one     1    3    2    0     0     0    0    0  0  ...     6.47     4.41   \n",
       "13  have     1    4    2    0     0     0    0    0  0  ...     6.71     3.83   \n",
       "14  this     1    4    1    0     0     0    0    0  0  ...     6.82     4.22   \n",
       "15  from     1    4    1    0     0     0    0    0  0  ...     6.63     4.18   \n",
       "16    by     1    2    0    0     0     0    0    0  0  ...     6.66     4.55   \n",
       "17   hot     1    3    1    0     0     0    0    0  0  ...     5.16     3.93   \n",
       "18  word     1    4    1    0     0     0    0    0  0  ...     5.26     3.64   \n",
       "19   but     1    3    1    0     0     0    0    0  0  ...     6.63     3.74   \n",
       "\n",
       "    ca_zipf  it_zipf  fr_zipf  pt_zipf  de_zipf  sv_zipf  pl_zipf  ru_zipf  \n",
       "0      4.22     4.26     5.79     6.73     4.57     4.66     4.51     3.89  \n",
       "1      3.12     3.67     3.70     3.94     3.93     3.89     3.82     3.62  \n",
       "2      3.67     4.24     4.12     4.38     4.56     4.50     4.40     3.98  \n",
       "3      5.87     4.00     4.02     4.01     4.36     4.32     4.39     3.74  \n",
       "4      3.78     3.94     3.83     3.99     6.49     4.01     5.57     3.76  \n",
       "5      4.73     4.86     4.60     5.62     4.88     4.94     4.98     4.71  \n",
       "6      6.18     4.98     6.71     5.17     4.95     4.93     5.98     4.49  \n",
       "7      3.67     4.22     4.05     4.32     4.29     4.42     4.42     3.94  \n",
       "8      4.24     4.38     4.40     4.67     4.58     4.52     4.52     4.22  \n",
       "9      3.42     3.76     3.60     3.76     3.83     3.82     3.93     3.45  \n",
       "10     4.75     4.59     4.27     4.47     4.72     5.04     4.51     4.09  \n",
       "11     4.36     4.34     4.26     4.82     4.67     4.77     4.41     4.41  \n",
       "12     4.30     4.63     4.53     4.90     4.65     4.62     5.49     4.40  \n",
       "13     3.67     4.02     3.83     4.00     4.14     4.15     4.23     3.71  \n",
       "14     4.10     4.42     4.15     4.43     4.51     4.48     4.49     4.10  \n",
       "15     3.90     4.26     4.15     4.67     4.40     4.47     4.55     4.02  \n",
       "16     4.64     4.72     4.52     4.78     4.72     4.94     6.26     4.54  \n",
       "17     3.70     4.19     4.05     4.09     4.16     4.93     4.32     3.57  \n",
       "18     3.28     3.71     3.57     3.88     3.90     3.82     3.87     3.34  \n",
       "19     3.55     3.90     5.28     4.01     4.05     4.17     4.28     3.56  \n",
       "\n",
       "[20 rows x 66 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('data/final.csv')\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5eeb9502143e217e56295c83298d15722c318a93c49e1a755647eb96066515fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
