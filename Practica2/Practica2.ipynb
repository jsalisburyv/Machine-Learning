{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0ee14f",
   "metadata": {},
   "source": [
    "# Pràctica 2\n",
    "\n",
    "L'objectiu d'aquesta segona pràctica es demostrar que heu assolit els conceptes que s'han explicar a l'assignatura i s'han practicat a les sessions presencials, relacionats amb el disseny i l'ús de xarxes neurals. \n",
    "\n",
    "Aquesta pràctica consta de 3 enunciats dels quals només heu de realitzar el que us hagi tocat per sorteig.\n",
    "\n",
    "**Condicions**\n",
    "1. El model que solucioni el problema estarà basat en xarxes neurals, aquestes s'han d'entrenar i avaluar emprant la llibreria _Pytorch_.\n",
    "2. Es demana que com a mínim s'avaluin 2 models diferents: un que ha d'estar creat per vosaltres i un altre que es basi en una xarxa ja existent. Evidentment es permeten modificacions de la ja existen per adaptar-ho al problema que es vol resoldre.\n",
    "3. El resultat del treball serà un informe on s'expliqui el procés que s'ha dut a terme per arribar a la que considereu que és millor solució. El document serà en format `pdf`. Podreu adjuntar una carpeta amb el codi i recursos que trobeu necessaris per comprovar la veracitat del que explicau al document.\n",
    "4. Aquest document ha de tenir un llenguatge formal i tècnic i ha d'estar correctament estructurat:\n",
    "    - Introducció al problema\n",
    "    - Solucions considerades (dades, característiques, models, mètriques)\n",
    "    - Experiments realitzats\n",
    "    - Resultats dels experiments\n",
    "    - Conclusions\n",
    "5. A més del document explicatiu s'ha d'adjuntar un fitxer amb els pesos del millor entrenament de cada una de les xarxes que heu emprat (la que heu dissenyat vosaltres i la que ja existia), de tal manera que el professor pugui validar els resultats sense haver de repetir l'entrenament. Sense l'adjunció d'aquests fitxers la pràctica no es podrà aprovar.\n",
    "6. Les dades depenen de cada un dels tres enunciats i les trobareu en el seu apartat.\n",
    "\n",
    "\n",
    "**Avaluació**\n",
    "\n",
    "- El treball es durà a terme en parelles.\n",
    "- El professor es reserva la possibilitat de convocar als grups a una revisió de la pràctica de forma presencial.\n",
    "- Només està permés emprar tècniques de disseny i entrenament vistes a classe.\n",
    "- Tot el que no està fet pels alumnes ha d'estar referenciat, en cas contrari es considerarà com una còpia.\n",
    "\n",
    "\n",
    "**Data d'entrega**\n",
    "\n",
    "- Aquest treball s'entrega dia 15 de gener.\n",
    "- Es realitzarà una tutoria dilluns 9 de gener a les 15:30.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47b046",
   "metadata": {},
   "source": [
    "## Enunciat 1: Classificació\n",
    "\n",
    "El problema que heu de resoldre en aquest cas és un problema de classificació amb el conjunt de dades _Horses or Human_ dataset [enllaç](https://laurencemoroney.com/datasets.html). És un conjunt de dades generat per ordinador en el que trobareu dues classes diferents: persones i cavalls (500 imatges de cavalls i 547 imatges de persones). També dos subconjunts de dades ja definits: entrenament  i validació. Les imatges tenen una mida de 300x300 pixels i es troben en RGB.\n",
    "\n",
    "A més de la feina de classificació i presentació dels resultats amb el conjunt de dades que es proporciona, també es demana que construiu un petit conjunt d'imatges (entre 10 i 20) de persones i cavalls reals com a conjunt de test i obtingueu les mesures rendiment adients per aquestes dades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388d6f4",
   "metadata": {},
   "source": [
    "**Exemples del dataset**\n",
    "<div style=\"display:flex\">\n",
    "     <div style=\"flex:1;padding-right:10px;\">\n",
    "          <img src=\"img/human01-16.png\" width=\"200\"/>\n",
    "     </div>\n",
    "     <div style=\"flex:1;padding-left:10px;\">\n",
    "          <img src=\"img/horse03-3.png\" width=\"200\"/>\n",
    "     </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf12ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e4de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.all_files = [f for f in os.listdir(self.path) if f.endswith('.png')]\n",
    "        self.labels = [f.split('-')[0] for f in self.all_files]\n",
    "        self.labels = [0 if x == 'horse' else 1 for x in self.labels]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.path, self.all_files[index])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # image = transforms.ToTensor()(image)\n",
    "        # image = image.permute(1, 2, 0)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de5efd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_normalization_values(path):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of the pixel values for each channel\n",
    "    in the images stored in the specified folder.\n",
    "    \"\"\"\n",
    "    red_values = []\n",
    "    green_values = []\n",
    "    blue_values = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.png'):\n",
    "            image = Image.open(os.path.join(path, file))\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            red, green, blue = image_np[:,:,0], image_np[:,:,1], image_np[:,:,2]\n",
    "\n",
    "            red_values.append(red)\n",
    "            green_values.append(green)\n",
    "            blue_values.append(blue)\n",
    "\n",
    "    red_mean = np.mean(red_values)/255\n",
    "    green_mean = np.mean(green_values)/255\n",
    "    blue_mean = np.mean(blue_values)/255\n",
    "\n",
    "    red_std = np.std(red_values)/255\n",
    "    green_std = np.std(green_values)/255\n",
    "    blue_std = np.std(blue_values)/255\n",
    "\n",
    "    return (red_mean, green_mean, blue_mean), (red_std, green_std, blue_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39054741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std = get_normalization_values(\"data/train/\")\n",
    "\n",
    "# mean = torch.tensor(mean)\n",
    "# std = torch.tensor(std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f916a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(path = \"data/train/\",  transform=transform)\n",
    "test_dataset = CustomDataset(path = \"data/validation/\", transform=transform)\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8938e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the first batch of data from the data loader\n",
    "# batch = next(iter(train_loader))\n",
    "# data, labels = batch\n",
    "\n",
    "# # Make a figure with subplots\n",
    "# fig, axs = plt.subplots(8, 8, figsize=(8, 8))\n",
    "\n",
    "# # Iterate over the data and labels and plot the images\n",
    "# for i, (data, label) in enumerate(zip(data, labels)):\n",
    "#     # Calculate the row and column indices for the subplot\n",
    "#     data = np.squeeze(data)\n",
    "\n",
    "#     row = i // 8\n",
    "#     col = i % 8\n",
    "#     # Plot the image on the corresponding subplot\n",
    "#     ax = axs[row, col]\n",
    "#     ax.imshow(data)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9069a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd12761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_CNN(nn.Module):\n",
    "    def __init__(self, channels, feature_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            self.block(channels, feature_size, 3, 2, 1),\n",
    "            self.block(feature_size, feature_size*2, 3, 2, 1),\n",
    "            self.block(feature_size*2, feature_size*4, 3, 2, 1),\n",
    "            self.block(feature_size*4, feature_size*8, 3, 2, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "    def block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a33079dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_FCN(nn.Module):\n",
    "    def __init__(self, channels, feature_size):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            self.block(channels, feature_size, 3, 2, 1), # 300x300 --> 150x150\n",
    "            self.block(feature_size, feature_size*2, 3, 2, 1), # 150x150 --> 75x75\n",
    "            self.block(feature_size*2, feature_size*4, 3, 2, 1), # 75x75 --> 38x38\n",
    "            self.block(feature_size*4, feature_size*8, 3, 2, 1), # 38x38 --> 19x19\n",
    "            self.block(feature_size*8, feature_size*4, 3, 2, 1), # 19x19 --> 10x10\n",
    "            self.block(feature_size*4, feature_size*2, 3, 2, 1), # 10x10 --> 5x5\n",
    "            self.block(feature_size*2, feature_size, 3, 2, 1), # 5x5 --> 3x3\n",
    "            self.block(feature_size, 1, 3, 2, 0), # 3x3 --> 2x2\n",
    "            nn.Flatten(),\n",
    "            nn.Softmax(dim=1)\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "    def block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c84fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "lr_CNN = 0.001\n",
    "lr_FCN = 0.01\n",
    "\n",
    "channels = 3\n",
    "\n",
    "feature_size_CNN = 2\n",
    "feature_size_FCN = 2\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "num_epochs_CNN = 50\n",
    "num_epochs_FCN = 50\n",
    "\n",
    "net_CNN = Net_CNN(channels, feature_size_CNN, num_classes).to(device)\n",
    "net_FCN = Net_FCN(channels, feature_size_FCN).to(device)\n",
    "\n",
    "net_CNN.apply(weights_init)\n",
    "net_FCN.apply(weights_init)\n",
    "\n",
    "criterion_CNN = nn.CrossEntropyLoss()\n",
    "criterion_FCN = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_CNN = torch.optim.SGD(net_CNN.parameters(), lr=lr_CNN, momentum=0.9)\n",
    "optimizer_FCN = torch.optim.SGD(net_FCN.parameters(), lr=lr_FCN, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41aa5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_FCN(\n",
      "  (main): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(8, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(4, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "Total number of parameters CNN:  1810\n",
      "Total number of parameters FCN:  3231\n"
     ]
    }
   ],
   "source": [
    "print(net_FCN)\n",
    "pytorch_total_params_CNN = sum(p.numel() for p in net_CNN.parameters())\n",
    "pytorch_total_params_FCN = sum(p.numel() for p in net_FCN.parameters())\n",
    "print(\"Total number of parameters CNN: \", pytorch_total_params_CNN)\n",
    "print(\"Total number of parameters FCN: \", pytorch_total_params_FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101cbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_train = SummaryWriter(f\"logs/train\")\n",
    "writer_test = SummaryWriter(f\"logs/test\")\n",
    "\n",
    "step_train = 0\n",
    "step_test = 0\n",
    "\n",
    "tr_loss = np.zeros((num_epochs_CNN))\n",
    "te_loss = np.zeros((num_epochs_CNN))\n",
    "\n",
    "tr_acc = np.zeros((num_epochs_CNN))\n",
    "te_acc = np.zeros((num_epochs_CNN))\n",
    "\n",
    "for epoch in range(num_epochs_CNN):\n",
    "\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    net_CNN.train()\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = torch.tensor(labels)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer_CNN.zero_grad()\n",
    "        outputs = net_CNN(data)\n",
    "        loss = criterion_CNN(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_CNN.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # compute the accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / len(labels)\n",
    "        train_acc += accuracy\n",
    "\n",
    "\n",
    "        writer_train.add_scalar(\"Training loss\", loss, global_step=step_train)\n",
    "        step_train += 1\n",
    "    \n",
    "    net_CNN.eval()\n",
    "    with torch.no_grad():\n",
    "        for j, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            labels = torch.tensor(labels)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net_CNN(data)\n",
    "            loss = criterion_CNN(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # compute the accuracy for this batch\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct / len(labels)\n",
    "            test_acc += accuracy\n",
    "\n",
    "            writer_test.add_scalar(\"Test loss\", loss, global_step=step_test)\n",
    "            step_test += 1\n",
    "\n",
    "    # compute the average loss and accuracy for each epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    test_loss /= len(test_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    tr_loss[epoch] = train_loss\n",
    "    te_loss[epoch] = test_loss\n",
    "    tr_acc[epoch] = train_acc\n",
    "    te_acc[epoch] = test_acc\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_CNN}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(tr_acc, label='Train accuracy')\n",
    "plt.plot(te_acc, label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00367ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walli\\AppData\\Local\\Temp\\ipykernel_7676\\660424454.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data, labels = data.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walli\\AppData\\Local\\Temp\\ipykernel_7676\\660424454.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data, labels = data.to(device), torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 51.5625, Test Loss: 50.0000, Train Acc: 29.4118, Test Acc: 32.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m test_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     16\u001b[0m net_FCN\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfor\u001b[39;00m i, (data, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     18\u001b[0m     data, labels \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), torch\u001b[39m.\u001b[39mtensor(labels)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m     optimizer_FCN\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\TFG\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\TFG\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\TFG\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\TFG\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [16], line 15\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     14\u001b[0m     image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_files[index])\n\u001b[1;32m---> 15\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\u001b[39m.\u001b[39;49mconvert(\u001b[39m'\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     16\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index]\n\u001b[0;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\TFG\\lib\\site-packages\\PIL\\Image.py:901\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    857\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    858\u001b[0m ):\n\u001b[0;32m    859\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    903\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    905\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\walli\\miniconda3\\envs\\TFG\\lib\\site-packages\\PIL\\ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m         )\n\u001b[0;32m    256\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 257\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_test = 0\n",
    "\n",
    "tr_loss = np.zeros((num_epochs_FCN))\n",
    "te_loss = np.zeros((num_epochs_FCN))\n",
    "\n",
    "tr_acc = np.zeros((num_epochs_FCN))\n",
    "te_acc = np.zeros((num_epochs_FCN))\n",
    "\n",
    "for epoch in range(num_epochs_FCN):\n",
    "\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    net_FCN.train()\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer_FCN.zero_grad()\n",
    "        outputs = net_FCN(data)\n",
    "    \n",
    "        labels = labels.unsqueeze(1)\n",
    "        print(outputs, labels)\n",
    "\n",
    "        loss = F.binary_cross_entropy(outputs,  labels.type(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer_FCN.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # compute the accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / len(labels)\n",
    "        train_acc += accuracy\n",
    "\n",
    "    net_FCN.eval()\n",
    "    with torch.no_grad():\n",
    "        for j, (data, labels) in enumerate(test_loader):\n",
    "            data, labels = data.to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "            optimizer_FCN.zero_grad()\n",
    "            outputs = net_FCN(data)\n",
    "        \n",
    "            labels = labels.unsqueeze(1)\n",
    "            loss = F.binary_cross_entropy(outputs,  labels.type(torch.float32))\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # compute the accuracy for this batch\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct / len(labels)\n",
    "            test_acc += accuracy\n",
    "\n",
    "    # compute the average loss and accuracy for each epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    test_loss /= len(test_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "    tr_loss[epoch] = train_loss\n",
    "    te_loss[epoch] = test_loss\n",
    "    tr_acc[epoch] = train_acc\n",
    "    te_acc[epoch] = test_acc\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_FCN}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(tr_acc, label='Train accuracy')\n",
    "plt.plot(te_acc, label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('TFG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a3f1cc4cb12d314308f93e8d7f4e89eeb6743400550d62b7d846e6683a05f7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
