{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "FaadnhbpCcsh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly7lrx-gCuLy"
   },
   "source": [
    "# Xarxes convolucionals\n",
    "\n",
    "L'objectiu d'avui és la creació d'una xarxa convolucional que obtengui com a mínim igual resultat que la xarxa completament connectada implementada la setmana anterior però amb menys paràmetres. Per poder realitzar comparacions directes emprarem el mateix conjunt de dades.\n",
    "\n",
    "Com objectius secundaris tenim:\n",
    "\n",
    "1. Aprenentatge de noves estratègies per evitar `overfitting`.\n",
    "2. Us d'un nou optimitzador.\n",
    "3. Visualització dels resultats dels filtres convolucionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "PwSoPhjXCvV9"
   },
   "outputs": [],
   "source": [
    "etiquetes = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions \n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # mitjana, desviacio tipica\n",
    "    ])\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch\n",
    "train = datasets.FashionMNIST('../data', train=True, download=True, transform=transform)\n",
    "test = datasets.FashionMNIST('../data', train=False, transform=transform)\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8i4Mg8KuD3r"
   },
   "source": [
    "## Definició de la xarxa\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "1. Definir la primera xarxa convolucional. A continuació teniu una llista de les capes que podeu emprar:\n",
    "\n",
    "\n",
    "- `Conv2d`: Capa convolucional en 2 dimensions. Com a paràmetres principals trobarem:\n",
    "\n",
    "  - in_channels: canals d'entrada.\n",
    "  - out_channels : canals de sortida (nombre de filtres).\n",
    "  - kernel_size: mida del filtre.\n",
    "  - stride: desplaçament del filtre. Típicament pren per valor 1.\n",
    "  - padding: ampliació de la imatge per evitar pèrdua de dimensionalitat.\n",
    "\n",
    "- `MaxPool2d`: Capa de max pooling. Aquesta capa no té paràmetres entrenables. Però si:\n",
    "\n",
    "  - kernel_size: Mida del filtre del qual es seleccionarà el màxim.\n",
    "  - stride: desplaçament del filtre.\n",
    "\n",
    "- `Dropout`: Dropout és un mètode de regularització (evitar `overfitting`) que aproxima l'entrenament d'un gran nombre de xarxes neuronals amb diferents arquitectures en paral·lel. Durant l'entrenament, una part de les sortides de la capa s'ignoren aleatòriament o s'abandonen. Això té l'efecte de fer que la capa sembli i es tracti com una capa amb un nombre diferent de nodes i connectivitat a la capa anterior. En efecte, cada actualització d'una capa durant l'entrenament es realitza amb una vista diferent de la capa configurada. Hem d'especificar quines capes tenen `dropout` de manera individual. Té un únic paràmetre amb valor per defecte $p=0.5$ Els valors típics d'aquest paràmetre varien entre $0.5$ i $0.8$.\n",
    "\n",
    "\n",
    "- `Linear`\n",
    "\n",
    "- `ReLU`\n",
    "\n",
    "\n",
    "2. Per posibilitar la visualització de les imatges passades per les capes convolucionals farem que funció `forward`tengui diverses sortides (diferents valors de `return`) un per cadda capa convolucional de la xarxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "IQvdRDtTHdRy"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (810403140.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn [61], line 6\u001B[1;36m\u001B[0m\n\u001B[1;33m    self.c1 = nn.Conv2d(in_channels=1, out_channels=8 kernel_size=5, stride=1, padding=0)\u001B[0m\n\u001B[1;37m                                                      ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convolution\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
    "        self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # MLP\n",
    "        self.l1 = nn.Linear(4*4*16, 64)\n",
    "        self.l2 = nn.Linear(64, len(etiquetes))\n",
    "        self.drop = nn.Dropout()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.c1(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        x = torch.flatten(x,1) # TODO: comprovar l'atribut shape de x un cop fet flatten\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.l2(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ISOL_hCk7g"
   },
   "source": [
    "## Entrenament\n",
    "\n",
    "Això no varia massa de la setmana anterior\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "1. Modificar la sortida de la xarxa, ara retorna diversos valors, encara que aquí només us interessa un."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "h9OLtpPzClch"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target, reduction='sum') \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
    " \n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum') \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBGKL43vsUnD"
   },
   "source": [
    "A continuació definim els paràmetres d'entrenament i el bucle principal:\n",
    "\n",
    "### Adam\n",
    "\n",
    "Aquesta setmana introduirem un nou algorisme d'optimització anomenat `Adam`. Fins ara hem emprat el descens del gradient (`SGD`). \n",
    "\n",
    "`Adam()` és un algorisme d'optimització amplament emprat, tal com el descens del gradient, és iteratiu. A la literatura trobam arguments que indiquen que, tot i que Adam convergeix més ràpidament, SGD  generalitza millor que Adam i, per tant, resulta en un rendiment final millor. \n",
    "\n",
    "[Més info](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)\n",
    "\n",
    "\n",
    "### Feina a fer:\n",
    "1. Mostrar el nombre de paràmetres de la xarxa (també a la xarxa de la setmana passada)\n",
    "```\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "```\n",
    "2. Dibuixar els gràfics de la funció de pèrdua amb les dues funcions d'optimització que coneixem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cNIBWqAwsVSb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147146\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 151.316345, Average: 2.364318\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 146.845474, Average: 2.294461\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 141.461685, Average: 2.210339\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 133.782318, Average: 2.090349\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 130.262192, Average: 2.035347\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 120.762856, Average: 1.886920\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 118.556549, Average: 1.852446\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 105.730621, Average: 1.652041\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 104.424614, Average: 1.631635\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 88.804756, Average: 1.387574\n",
      "\n",
      "Train set: Average loss: 1.9281\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.3916, Accuracy: 5900/10000 (59%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 103.323265, Average: 1.614426\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 98.387657, Average: 1.537307\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 79.735756, Average: 1.245871\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 80.398590, Average: 1.256228\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 77.881920, Average: 1.216905\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 81.191681, Average: 1.268620\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 73.130531, Average: 1.142665\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 66.529465, Average: 1.039523\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 71.913162, Average: 1.123643\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 71.266167, Average: 1.113534\n",
      "\n",
      "Train set: Average loss: 1.1985\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9610, Accuracy: 6656/10000 (67%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 72.735786, Average: 1.136497\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 73.168114, Average: 1.143252\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 52.299259, Average: 0.817176\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 63.654221, Average: 0.994597\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 66.515594, Average: 1.039306\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 66.754349, Average: 1.043037\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 62.899246, Average: 0.982801\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 64.156937, Average: 1.002452\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 65.113182, Average: 1.017393\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 62.123966, Average: 0.970687\n",
      "\n",
      "Train set: Average loss: 0.9661\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8468, Accuracy: 6935/10000 (69%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 60.344757, Average: 0.942887\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 64.397240, Average: 1.006207\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 46.477013, Average: 0.726203\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 61.055450, Average: 0.953991\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 58.373161, Average: 0.912081\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 59.003765, Average: 0.921934\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 55.007637, Average: 0.859494\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 57.850933, Average: 0.903921\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 46.663876, Average: 0.729123\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 53.926537, Average: 0.842602\n",
      "\n",
      "Train set: Average loss: 0.8660\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7845, Accuracy: 7106/10000 (71%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 52.530811, Average: 0.820794\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 58.020054, Average: 0.906563\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 42.892696, Average: 0.670198\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 57.480927, Average: 0.898139\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 52.650581, Average: 0.822665\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 55.709484, Average: 0.870461\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 50.048840, Average: 0.782013\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 54.196468, Average: 0.846820\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 50.664761, Average: 0.791637\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 58.161259, Average: 0.908770\n",
      "\n",
      "Train set: Average loss: 0.8092\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7449, Accuracy: 7215/10000 (72%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 49.392265, Average: 0.771754\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 54.617531, Average: 0.853399\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 39.649765, Average: 0.619528\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 51.523361, Average: 0.805053\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 48.011925, Average: 0.750186\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 55.601196, Average: 0.868769\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 49.022758, Average: 0.765981\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 48.432087, Average: 0.756751\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 51.384274, Average: 0.802879\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 53.268074, Average: 0.832314\n",
      "\n",
      "Train set: Average loss: 0.7683\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7157, Accuracy: 7331/10000 (73%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 43.588287, Average: 0.681067\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 51.214905, Average: 0.800233\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 38.545837, Average: 0.602279\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 53.681080, Average: 0.838767\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 52.709446, Average: 0.823585\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 54.002266, Average: 0.843785\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 44.103058, Average: 0.689110\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 47.997322, Average: 0.749958\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 47.904976, Average: 0.748515\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 48.020576, Average: 0.750322\n",
      "\n",
      "Train set: Average loss: 0.7401\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6932, Accuracy: 7403/10000 (74%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 45.617123, Average: 0.712768\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 50.626827, Average: 0.791044\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 34.399899, Average: 0.537498\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 50.191540, Average: 0.784243\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 46.486843, Average: 0.726357\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 54.743118, Average: 0.855361\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 47.637115, Average: 0.744330\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 46.656815, Average: 0.729013\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 45.812336, Average: 0.715818\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 51.641312, Average: 0.806895\n",
      "\n",
      "Train set: Average loss: 0.7130\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6728, Accuracy: 7482/10000 (75%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 39.788937, Average: 0.621702\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 45.524914, Average: 0.711327\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 35.544865, Average: 0.555389\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 50.973984, Average: 0.796468\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 42.468864, Average: 0.663576\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 50.476219, Average: 0.788691\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 43.663555, Average: 0.682243\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 45.403057, Average: 0.709423\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 44.011192, Average: 0.687675\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 48.253185, Average: 0.753956\n",
      "\n",
      "Train set: Average loss: 0.6921\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6567, Accuracy: 7551/10000 (76%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 40.376026, Average: 0.630875\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 44.812710, Average: 0.700199\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 29.264954, Average: 0.457265\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 48.844326, Average: 0.763193\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 45.161915, Average: 0.705655\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 52.477383, Average: 0.819959\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 45.233006, Average: 0.706766\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 42.219971, Average: 0.659687\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 40.099548, Average: 0.626555\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 50.233440, Average: 0.784898\n",
      "\n",
      "Train set: Average loss: 0.6764\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6428, Accuracy: 7582/10000 (76%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 40.780342, Average: 0.637193\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 44.607082, Average: 0.696986\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 30.139463, Average: 0.470929\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 46.498394, Average: 0.726537\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 45.372639, Average: 0.708947\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 46.643147, Average: 0.728799\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 42.921448, Average: 0.670648\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 43.116615, Average: 0.673697\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 42.156082, Average: 0.658689\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 47.803436, Average: 0.746929\n",
      "\n",
      "Train set: Average loss: 0.6626\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6287, Accuracy: 7655/10000 (77%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 35.807686, Average: 0.559495\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 39.693630, Average: 0.620213\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 29.251427, Average: 0.457054\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 46.226959, Average: 0.722296\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 44.958920, Average: 0.702483\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 50.938869, Average: 0.795920\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 44.174522, Average: 0.690227\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 43.325661, Average: 0.676963\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 37.924072, Average: 0.592564\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 40.780880, Average: 0.637201\n",
      "\n",
      "Train set: Average loss: 0.6475\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6164, Accuracy: 7691/10000 (77%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 38.814346, Average: 0.606474\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 42.493614, Average: 0.663963\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 29.945910, Average: 0.467905\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 46.179813, Average: 0.721560\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 47.723995, Average: 0.745687\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 45.717003, Average: 0.714328\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 42.148918, Average: 0.658577\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 40.765411, Average: 0.636960\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 43.481693, Average: 0.679401\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 40.718937, Average: 0.636233\n",
      "\n",
      "Train set: Average loss: 0.6353\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6050, Accuracy: 7772/10000 (78%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 35.639355, Average: 0.556865\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 40.188732, Average: 0.627949\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 30.797342, Average: 0.481208\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 43.162060, Average: 0.674407\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 40.962315, Average: 0.640036\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 47.291698, Average: 0.738933\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 39.131664, Average: 0.611432\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 39.255318, Average: 0.613364\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 41.462227, Average: 0.647847\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 49.672646, Average: 0.776135\n",
      "\n",
      "Train set: Average loss: 0.6228\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5957, Accuracy: 7782/10000 (78%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 35.994678, Average: 0.562417\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 40.530506, Average: 0.633289\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 26.585537, Average: 0.415399\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 43.254910, Average: 0.675858\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 41.136471, Average: 0.642757\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 51.459984, Average: 0.804062\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 39.361839, Average: 0.615029\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 37.397488, Average: 0.584336\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 43.106972, Average: 0.673546\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 43.506126, Average: 0.679783\n",
      "\n",
      "Train set: Average loss: 0.6131\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5857, Accuracy: 7828/10000 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = False\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epochs = 15\n",
    "lr =0.00001\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr) #momentum\n",
    "\n",
    "# Guardam el valor de peèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch]  = test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjeMWK8cJkqN"
   },
   "source": [
    "## Resultats\n",
    "\n",
    "Aquí visualitzarem els resultats d'aprenentatge de la xarxa. \n",
    "\n",
    "### Feina a fer:\n",
    "\n",
    "1. Fer una predicció del primer _batch_ del conjunt de _test_.\n",
    "2. Visualitzar una imatge del _batch_ i posar la predicció i el groun truth com a títol de la imatge.\n",
    "3. Visualitzar el resultat de la mateixa imatge passada per tots els filtres de cada convolució de la vostra xarxa.\n",
    "4. **Extra**: Fer la matriu de confusió de les 10 classes per poder entendre el que no estau fent bé (la xarxa no està fent bé).\n",
    "\n",
    "A tenir en compte:\n",
    "\n",
    "#### Subplots\n",
    "\n",
    "Per fer graelles d'imatges podeu empar la funció `subplots`. Més [informació](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)\n",
    "\n",
    "#### Device\n",
    "\n",
    "Si heu emprat _GPU_ per accelerar el procés d'entrenament, els resultats que obtenim de la xarxa també seràn a la _GPU_. **Pytorch** proporciona la funció `cpu()` que retorna una còpia d'aquest objecte a la memòria de la CPU.\n",
    "\n",
    "#### Detach\n",
    "Per poder operar amb els resultats de la predicció emprarem la funció `detach` que retorna un nou Tensor \"separat\" del graf (xarxa) en curs.\n",
    "\n",
    "Per tant per transformar el tensor que retorna la xarxa en un array de la lliberia _Numpy_ caldria fer el següent:\n",
    "\n",
    "  ```\n",
    "  resultat_np = resultat.detach().numpy()\n",
    "  ```\n",
    "Si a més hem executat l'entrenament en _GPU_:\n",
    "  ```\n",
    "  resultat_np = resultat.cpu().detach().numpy()\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "SYKUppOc_4JE"
   },
   "outputs": [],
   "source": [
    "def generador(loader):\n",
    "  for data, target in test_loader:\n",
    "    yield data, target\n",
    "\n",
    "\n",
    "#TODO "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
