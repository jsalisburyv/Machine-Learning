{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gaussian Mixture Models\n",
    "\n",
    "En aquesta pràctica treballarem amb la tècnica de Gaussian Mixture Models que ens pot servir tant per realitzar agrupaments (_clustering_) o per generar noves mostres del nostre conjunt de dades. Treballarem amb el _dataset_ dels digits.\n",
    "\n",
    "Les passes que s'han de realitzar són les següents:\n",
    "\n",
    "1. Càrrega de dades.\n",
    "2. Reducció de la dimensionalitat.\n",
    "3. Selecció del model: parametrització.\n",
    "4. Generació de nous exemples.\n",
    "5. Extra: Validació dels nous exemples.\n",
    "\n",
    "## Reducció de la dimensionalitat\n",
    "\n",
    "Una de les tècniques més emprades per reduir la informació de les dades amb les quals tractam és l'anàlisi de components principals (PCA). [enllaç](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA)\n",
    "\n",
    "Aquesta tècnica ens permet descompondre les dades d'entrada en un conjunt ortogonal de components que expliquen la màxima quantitat de variància. La implementació de _Scikit_ ens permet seleccionar entre dues opcions: un nombre de components fixat o el nombre de components mínim per explicar una quantitat de variància fixada.\n",
    "\n",
    "Una tècnica existent per tal d'obtenir un bon equilibri entre la informació que perdem i el nombre de components que seleccionem, és dibuixar la **variància explicada acumulada** segons el nombre de components i seleccionar allà on veiem que la corba torna horitzontal. Aquesta tècnica es coneix amb el nom de la _regla del colze_.\n",
    "\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "En primer lloc, heu de carregar el conjunt de dades dels [digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html).\n",
    "\n",
    "En segon lloc, heu d'aplicar PCA a les dades mantenint totes les components i realitzar un gràfic que mostri la variància acumulada segons el nombre de components. Amb aquest gràfic podrem decidir quantes components mantenir. Tornar a aplicar PCA, aquest cop, amb el nombre de components seleccionades en la passa anterior.\n",
    "\n",
    "Per calcular la variància segons el nombre de components consulteu l'atribut `explained_variance_`. Per obtenir la suma acumulada d'un array `numpy` podeu emprar el mètode `cumsum`.\n",
    "\n",
    "Com podeu suposar, seleccionar el nombre de components d'acord amb un gràfic no és la manera més pràctica de fer-ho. Heu de fer una funció que decideixi quantes components són necessaries \"per aplanar la corva\".\n",
    "\n",
    "\n",
    "## Selecció del nombre de gaussianes de la mixtura.\n",
    "\n",
    "La següent passa consisteix a construir una mixtura de Gaussianes que s'adapti a les nostres dades. Però abans hem de trobar el paràmetre principal d'aquest mètode, que és el nombre de gaussianes a seleccionar. Tal com ho hem fet amb el PCA aquí també realitzarem aquesta selecció aplicant un criteri gràfic.\n",
    "\n",
    "La propia classe de _Scikit_ de Gaussian Mixture Model (GaussianMixture) té implementat un mètode anomenat Bayesian information criterion (`bic`) que ens dona un criteri de com el model s'adapta a un conjunt de dades, com menor és el seu valor millor s'adapta el model a les dades [enllaç](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn-mixture-gaussianmixture).\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "Visualitzar el criteri bic pel rang:`[1,10]` gaussianes. Cercar visualment el mínim local.\n",
    "\n",
    "## Generació de nous exemples\n",
    "\n",
    "Un cop hem seleccionat el nombre de gaussianes, podem ajustar el model i simplement usant el mètode `sample` de la classe `GaussianMixture` podrem generar noves mostres del conjunt de dades.\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "Generar 100 mostres i visualitzar-les. Per això, has d'emprar el mètode `sample` de la classe `GaussianMixture` i després desfer la projecció del PCA amb el mètode `inverse_transform`.\n",
    "\n",
    "## Validació dels resultats obtinguts\n",
    "\n",
    "Com ho faries per validar que el nou conjunt generat és similar al conjunt original?\n",
    "\n",
    "## Extra, extra!\n",
    "\n",
    "Si us sobra temps, és molt interessant fer aquesta feina amb el conjunt de dades de les cares en entorns reals. [enllaç](https://scikit-learn.org/0.16/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people).\n",
    "\n",
    "## Notes\n",
    "\n",
    "Més informacions:\n",
    "\n",
    "\n",
    "* PCA, lectura en 6 minuts [enllaç](https://medium.com/analytics-vidhya/principal-component-analysis-pca-558969e63613)\n",
    "\n",
    "* Tutorial PCA [enllaç](http://www.cs.cmu.edu/~elaw/papers/pca.pdf)\n",
    "\n",
    "* Bayesian Information Criterion [enllaç](https://stanfordphd.com/BIC.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
