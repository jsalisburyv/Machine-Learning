{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1ozwAAiAyNHA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transferència de coneixement\n",
    "\n",
    "L'objectiu d'avui és aprendre com podem emprar arquitectures ja existents per resoldre els nostres problemes. \n",
    "\n",
    "Com objectius secundaris tenim:\n",
    "\n",
    "1. Conèixer un nou conjunt de dades\n",
    "2. Entendre en profunditat com és una de les arquitecures més famoses.\n",
    "3. Guardar i carregar xarxes neuronals\n",
    "\n",
    "## Dades\n",
    "\n",
    "El conjunt de dades [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) consta de 60.000 imatges en color de 32x32 pixels etiquetades en 10 classes, amb 6.000 imatges per classe. Hi ha 50.000 imatges d'entrenament i 10.000 imatges de _test_.\n",
    "\n",
    "### Feina a fer:\n",
    "\n",
    "1. Adaptar la mateixa xarxa que vareu desenvolupar la setmana anterior per emprar aquest conjunt de dades. `Grayscale` és una funció que transforma imatges a escala de grisos, la podem emprar dins la nostra composició de transformacions.\n",
    "\n",
    "Si voleu normalitzar les dades, a continuació teniu els valors ja calculats:\n",
    "\n",
    "  - mitjana: (0.4914, 0.4822, 0.4465)\n",
    "  - desviació típica: (0.247, 0.243, 0.261)\n",
    "\n",
    "Una altra funció que pot ser útil és `Resize(mida_desti)` que rep un enter com a paràmetre (la mida final).\n"
   ],
   "metadata": {
    "id": "sMy35hII78XT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions \n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)), # mitjana, desviacio tipica\n",
    "    transforms.Resize(256)\n",
    "    ])\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch\n",
    "train = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "test = datasets.CIFAR10('../data', train=False, transform=transform)\n",
    "\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, test_batch_size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjdicviT7-dg",
    "outputId": "2b758f54-06ba-4d2c-ae2d-f90bd36a2570"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear_1 = nn.Linear(7*7*32, 128) #7 * 7 * 64, 120)\n",
    "        self.linear_2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv_1(x)\n",
    "        x = self.max_pool2d(y)\n",
    "        z = self.conv_2(x)\n",
    "        x = self.max_pool2d(z)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output, y, z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    model.train()\n",
    "    loss_v = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data)\n",
    "        loss = F.cross_entropy(output, target, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        loss_v += loss.item()\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
    "    return loss_v\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output,_ , _ = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='mean')\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters  207210\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x131072 and 1568x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 23>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Bucle d'entrenament\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, epochs):\n\u001B[1;32m---> 24\u001B[0m     train_l[epoch] \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     test_l[epoch]  \u001B[38;5;241m=\u001B[39m test(model, device, test_loader)\n",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, device, train_loader, optimizer, epoch, log_interval, verbose)\u001B[0m\n\u001B[0;32m      5\u001B[0m data, target \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      6\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m----> 7\u001B[0m output, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(output, target, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Soluciones\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     16\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_pool2d(z)\n\u001B[0;32m     17\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear_1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m     20\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Soluciones\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Soluciones\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (64x131072 and 1568x128)"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "epochs = 15\n",
    "lr =0.00001\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # !!!\n",
    "\n",
    "print(\"Parameters \",pytorch_total_params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de peèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch]  = test(model, device, test_loader)\n",
    "#567764"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer learning (Definició de la xarxa)\n",
    "\n",
    "En aquesta pràctica aplicarem la tècnica de _transfer learning_ a partir d'una de les xarxes més conegudes en el camp de visió per computador: [**AlexNet**](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). (ImageNet Classification with Deep Convolutional Neural Network, 2012).\n",
    "\n",
    "_Pytorch_ ens permet emprar les xarxes més conegudes de manera molt senzilla. [Més informació](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Per xarxes no tan conegudes podem guardar i carregar els models de manera molt senzilla: [Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "Anem a descarregar-la i a analitzar-la. En aquest cas no només ens baixam la seva arquitectura, també els pesos resultants de l'entrenament."
   ],
   "metadata": {
    "id": "dty0xrAh71Qw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "alex = models.alexnet(weights='DEFAULT')\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Arquitectura AlexNet\")\n",
    "print(\"-\"*50)\n",
    "print(alex)"
   ],
   "metadata": {
    "id": "xaoFxi7cygHX"
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\jonny/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "10.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "35.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "60.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "82.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "96.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hi ha diverses maneres de realitzar aquesta tècnica les dues més conegudes són:\n",
    "\n",
    " - **\"Congelar\"** els pesos de la part d'extracció de característiques i crear un nou classificador. Això implica que només entrenam una part de la xarxa.\n",
    " - **Reentrenar tota la xarxa**.\n",
    "\n",
    " Per tal d'evitar el reentrenament necessitam canviar el valor de l'atribut  `requires_grad` al valor `False`. Aquest atribut és propietat de cada tensor. Podem recorrer els tensors mitjançant el següent codi:\n",
    " ```\n",
    "for param in alex.features.parameters():\n",
    "    param.requires_grad = False\n",
    " ```\n",
    "\n",
    " ### Feina a fer:\n",
    "\n",
    " 1. Carregar la xarxa AlexNet i seleccionar la part d'extracció de característiques.\n",
    " 2. Definir un entorn seqüencial on implementarem el classificador de la xarxa.\n",
    " 3. Realitzar un entrenament i comparar els resultats amb el primer entrenament (xarxa pròpia): comparar rendiment (accuracy) però també temps dedicat a entrenar i nombre de paràmetres.\n",
    " 4. Provar de guardar la vostra xarxa i tornar-la a carregar. Per classificar una imatge del conjunt de test."
   ],
   "metadata": {
    "id": "asepjghw2xED"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO Congelar\n",
    "for param in alex.features.parameters():\n",
    "   param.requires_grad = False\n",
    "\n",
    "my_net =  nn.Sequential(alex.features, nn.Flatten(1,-1), nn.Linear(12544, 512), nn.Softmax(), nn.Linear(512, 10), nn.Softmax())"
   ],
   "metadata": {
    "id": "ZrzR1-4Ny3hx"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrenament\n",
    "\n",
    "[shhht](https://github.com/tqdm/tqdm) si voleu canviar el resum de l'entrenament per una barra de progrés"
   ],
   "metadata": {
    "id": "7ZH80zEa8IPW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target, reduction='mean') \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
    " \n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='mean') \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss"
   ],
   "metadata": {
    "id": "eJiXfzTM7e8d"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "use_cuda = True\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epochs = 5#¿?\n",
    "lr = 0.000001#¿?\n",
    "\n",
    "model = my_net.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # !!!\n",
    "\n",
    "print(\"Parameters \", pytorch_total_params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)#¿?\n",
    "\n",
    "# Guardam el valor de pèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch]  = test(model, device, test_loader)\n"
   ],
   "metadata": {
    "id": "llV5gCGU7jIT"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters  6428170\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.302620, Average: 0.035978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonny\\miniconda3\\envs\\Soluciones\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 2.302201, Average: 0.035972\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 2.302356, Average: 0.035974\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 2.303329, Average: 0.035990\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 2.302071, Average: 0.035970\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 2.302792, Average: 0.035981\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 2.302298, Average: 0.035973\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 2.301694, Average: 0.035964\n",
      "\n",
      "Train set: Average loss: 0.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 1651/10000 (17%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.302103, Average: 0.035970\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.301553, Average: 0.035962\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.301567, Average: 0.035962\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.302660, Average: 0.035979\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.301182, Average: 0.035956\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.302164, Average: 0.035971\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.301393, Average: 0.035959\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.300735, Average: 0.035949\n",
      "\n",
      "Train set: Average loss: 0.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 2138/10000 (21%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.301347, Average: 0.035959\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.300744, Average: 0.035949\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.300756, Average: 0.035949\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.301910, Average: 0.035967\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.300411, Average: 0.035944\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.301462, Average: 0.035960\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 2.300743, Average: 0.035949\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 2.300172, Average: 0.035940\n",
      "\n",
      "Train set: Average loss: 0.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 2170/10000 (22%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.300870, Average: 0.035951\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 2.300307, Average: 0.035942\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 2.300066, Average: 0.035939\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 2.301461, Average: 0.035960\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 2.299921, Average: 0.035936\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 2.300952, Average: 0.035952\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 2.300346, Average: 0.035943\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 2.299755, Average: 0.035934\n",
      "\n",
      "Train set: Average loss: 0.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 2100/10000 (21%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 2.300488, Average: 0.035945\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 2.299958, Average: 0.035937\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 2.299580, Average: 0.035931\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 2.301166, Average: 0.035956\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 2.299604, Average: 0.035931\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 2.300653, Average: 0.035948\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 2.300063, Average: 0.035938\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 2.299475, Average: 0.035929\n",
      "\n",
      "Train set: Average loss: 0.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 2148/10000 (21%)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.title(\"Resultats de l'entrenament\")\n",
    "plt.plot(range(1, (epochs + 1)), train_l,  c=\"red\", label=\"train\")\n",
    "plt.plot(range(1,  (epochs + 1)), test_l,  c=\"green\", label=\"test\")\n",
    "plt.legend();"
   ],
   "metadata": {
    "id": "AJzao3Z7Jlc_"
   },
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  }
 ]
}
